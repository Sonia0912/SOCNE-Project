{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V100",
      "collapsed_sections": [
        "WGFsxu283gPy",
        "y7zRD8k6Lqav",
        "3nnDtCZAL5un",
        "O9B7YTbhjCPI",
        "k2b5UGQFBVWN",
        "g-sfv6MMQ3Lm",
        "e_AbpeUw4rXa",
        "96V81zFT6pj5",
        "RfKoHSz76tlP",
        "K9ykkn5aMDxc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Risk or Opportunity: Public Discourse on the Dangers of AI**\n",
        "Sonia Nicoletti (A12922110)\n",
        "\n",
        "Social Media and Social Network Analysis Project"
      ],
      "metadata": {
        "id": "i_G5qLtnPmvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing the dataset**"
      ],
      "metadata": {
        "id": "WGFsxu283gPy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNLdsyI448oW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "!pip install bertopic\n",
        "from bertopic.representation import ZeroShotClassification\n",
        "from bertopic import BERTopic\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, cohen_kappa_score\n",
        "import chardet\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the dataset\n",
        "file_name = 'risks_comments.csv'\n",
        "df = pd.read_csv(file_name)\n",
        "len(df)"
      ],
      "metadata": {
        "id": "yP-iLEZX5F91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sentiment analysis of AI videos**"
      ],
      "metadata": {
        "id": "i-6VR30v3yEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 1: cardiffnlp/twitter-roberta-base-sentiment-latest"
      ],
      "metadata": {
        "id": "y7zRD8k6Lqav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 -> Negative; 1 -> Neutral; 2 -> Positive\n",
        "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    try:\n",
        "        result = sentiment_pipe(text)[0]\n",
        "        return result['label'], result['score']\n",
        "    except Exception as e:\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "0NDHqSIH5K0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with 100 entries\n",
        "%%time\n",
        "df_s = df.sample(n=100)\n",
        "df_s['sentiment_label'], df_s['sentiment_score'] = zip(*df_s['textOriginal'].apply(analyze_sentiment))\n",
        "df_s.head()"
      ],
      "metadata": {
        "id": "H9hsX2q65WUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting all the entries\n",
        "%%time\n",
        "df['sentiment_label'], df['sentiment_score'] = zip(*df['textOriginal'].apply(analyze_sentiment))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9pDo7rs-5Yed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relabelling the columns\n",
        "df['manual_classification'] = df['goldstandard'].replace({0: 'negative', 1: 'neutral', 2: 'positive'})\n",
        "\n",
        "# Printing the scores\n",
        "print(\"F1 Score is: \",  f1_score(df.manual_classification.astype(str), df.sentiment_label.astype(str), labels=['negative', 'neutral', 'positive'], average='weighted'))\n",
        "print(\"Precision is: \", precision_score(df.manual_classification.astype(str), df.sentiment_label.astype(str),  labels=['negative', 'neutral', 'positive'], average='weighted') )\n",
        "print(\"Recall is: \",    recall_score(df.manual_classification.astype(str), df.sentiment_label.astype(str),  labels=['negative', 'neutral', 'positive'], average='weighted')  )\n",
        "print(\"Kappa is: \",    cohen_kappa_score(df.manual_classification.astype(str), df.sentiment_label.astype(str)))"
      ],
      "metadata": {
        "id": "guV5eD2NEY6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 2: distilbert-base-multilingual-cased-sentiments-student"
      ],
      "metadata": {
        "id": "3nnDtCZAL5un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# labels: 'positive', 'neutral', 'negative'\n",
        "sentiment_pipe = pipeline(model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", return_all_scores=True)\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    try:\n",
        "        result = sentiment_pipe(text)[0]\n",
        "        return result[0]['label'], result[0]['score'], result[1]['label'], result[1]['score'], result[2]['label'], result[2]['score']\n",
        "    except Exception as e:\n",
        "        return None, None, None, None, None, None"
      ],
      "metadata": {
        "id": "wwOyUOwQK4wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with 100 entries\n",
        "df_s = df.sample(n=100)\n",
        "df_s['positive'], df_s['positive_score'], df_s['neutral'], df_s['neutral_score'], df_s['negative'], df_s['negative_score'] = zip(*df_s['textOriginal'].apply(analyze_sentiment))\n",
        "df_s.head()"
      ],
      "metadata": {
        "id": "x5oHKRrr9wLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model on all the data\n",
        "df['positive'], df['positive_score'], df['neutral'], df['neutral_score'], df['negative'], df['negative_score'] = zip(*df['textOriginal'].apply(analyze_sentiment))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "h21tsDZU92-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a column for the sentiment with the highest score\n",
        "def determine_label(row):\n",
        "    max_score = ''\n",
        "    if row['positive_score'] is not None and row['neutral_score'] is not None and row['negative_score'] is not None :\n",
        "      max_score = max(row['positive_score'], row['neutral_score'], row['negative_score'])\n",
        "    if max_score == row['positive_score']:\n",
        "        return 'positive'\n",
        "    elif max_score == row['neutral_score']:\n",
        "        return 'neutral'\n",
        "    elif max_score == row['negative_score']:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'NA'\n",
        "\n",
        "df['sentiment_label'] = df.apply(determine_label, axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "rmIdJ80_90M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relabelling the columns\n",
        "df['manual_classification'] = df['goldstandard'].replace({0: 'negative', 1: 'neutral', 2: 'positive'})\n",
        "\n",
        "# Printing the scores\n",
        "print(\"F1 Score is: \",  f1_score(df.manual_classification.astype(str), df.sentiment_label.astype(str), labels=['negative', 'neutral', 'positive'], average='weighted'))\n",
        "print(\"Precision is: \", precision_score(df.manual_classification.astype(str), df.sentiment_label.astype(str),  labels=['negative', 'neutral', 'positive'], average='weighted') )\n",
        "print(\"Recall is: \",    recall_score(df.manual_classification.astype(str), df.sentiment_label.astype(str),  labels=['negative', 'neutral', 'positive'], average='weighted')  )\n",
        "print(\"Kappa is: \",    cohen_kappa_score(df.manual_classification.astype(str), df.sentiment_label.astype(str)))"
      ],
      "metadata": {
        "id": "0IkFXIK1gu3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 3: bertweet-base-sentiment-analysis"
      ],
      "metadata": {
        "id": "O9B7YTbhjCPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# labels: 'POS', 'NEG', 'NEU'\n",
        "sentiment_pipe = pipeline(\"text-classification\", model='finiteautomata/bertweet-base-sentiment-analysis')\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    try:\n",
        "        result = sentiment_pipe(text)[0]\n",
        "        return result['label'], result['score']\n",
        "    except Exception as e:\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "uVwTW94ZhzcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with 15000 entries\n",
        "df_s = df.sample(n=15000)\n",
        "df_s['Bertweet_sentiment'], df_s['Bertweet_score'] = zip(*df_s['textOriginal'].apply(analyze_sentiment))\n",
        "df_s.head()"
      ],
      "metadata": {
        "id": "K6LERjVt-ope"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model on all the data\n",
        "df['Bertweet_sentiment'], df['Bertweet_score'] = zip(*df['textOriginal'].apply(analyze_sentiment))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "F71V8pjy-qQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relabelling the columns (with two lables: negative and neutral)\n",
        "df['manual_classification'] = df['goldstandard'].replace({0: 'negative', 1: 'neutral', 2: 'neutral'})\n",
        "df['sentiment_label'] = df['Bertweet_sentiment'].replace({'NEG': 'negative', 'NEU': 'neutral', 'POS': 'neutral'})\n",
        "\n",
        "# Printing the scores\n",
        "print(\"F1 Score is: \",  f1_score(df.manual_classification.astype(str), df.sentiment_label.astype(str), labels=['negative', 'neutral'], average='weighted'))\n",
        "print(\"Precision is: \", precision_score(df.manual_classification.astype(str), df.sentiment_label.astype(str),  labels=['negative', 'neutral'], average='weighted') )\n",
        "print(\"Recall is: \",    recall_score(df.manual_classification.astype(str), df.sentiment_label.astype(str),  labels=['negative', 'neutral'], average='weighted')  )\n",
        "print(\"Kappa is: \",    cohen_kappa_score(df.manual_classification.astype(str), df.sentiment_label.astype(str)))"
      ],
      "metadata": {
        "id": "wHIw2bztkRqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relabelling the columns\n",
        "df['manual_classification'] = df['goldstandard'].replace({0: 'negative', 1: 'neutral', 2: 'positive'})\n",
        "df['sentiment_label'] = df['Bertweet_sentiment'].replace({'NEG': 'negative', 'NEU': 'neutral', 'POS': 'positive'})\n",
        "\n",
        "# Printing the scores\n",
        "print(\"F1 Score is: \",  f1_score(df.manual_classification.astype(str), df.sentiment_label.astype(str), labels=['negative', 'neutral', 'positive'], average='weighted'))\n",
        "print(\"Precision is: \", precision_score(df.manual_classification.astype(str), df.sentiment_label.astype(str),  labels=['negative', 'neutral', 'positive'], average='weighted') )\n",
        "print(\"Recall is: \",    recall_score(df.manual_classification.astype(str), df.sentiment_label.astype(str),  labels=['negative', 'neutral', 'positive'], average='weighted')  )\n",
        "print(\"Kappa is: \",    cohen_kappa_score(df.manual_classification.astype(str), df.sentiment_label.astype(str)))"
      ],
      "metadata": {
        "id": "CA3xsXiBlI9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 4: bart-large-mnli"
      ],
      "metadata": {
        "id": "k2b5UGQFBVWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "labels = [ \"dangerous\", \"neutral\" ]\n",
        "\n",
        "def analyse_sentiment(text):\n",
        "    output = pipe(text,\n",
        "                  candidate_labels = labels,\n",
        "                  hypothesis_template = 'This comment says AI is {}'\n",
        "                 )\n",
        "    labs = output[\"labels\"]\n",
        "    return labs[0]"
      ],
      "metadata": {
        "id": "AHls_fnzBYmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with 100 entries\n",
        "%%time\n",
        "df_s = df.sample(n=100)\n",
        "df_s[\"y_pred\"] = df_s.textOriginal.apply(analyse_sentiment)"
      ],
      "metadata": {
        "id": "U1T2bnRNB0U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model on all the data\n",
        "%%time\n",
        "df[\"y_pred\"] = df.textOriginal.apply(analyse_sentiment)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "hb0kNpTKCuRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relabelling the columns\n",
        "df['manual_classification'] = df['goldstandard'].replace({0: 'negative', 1: 'neutral', 2: 'positive'})\n",
        "df['sentiment_label'] = df['y_pred'].replace({'dangerous': 'negative', 'neutral': 'neutral', 'beneficial': 'positive'})\n",
        "\n",
        "# Printing the scores\n",
        "print(\"F1 Score is: \",  f1_score(df.manual_classification.astype(str), df.sentiment_label.astype(str), labels=['negative', 'neutral', 'positive'], average='weighted'))\n",
        "print(\"Precision is: \", precision_score(df.manual_classification.astype(str), df.sentiment_label.astype(str),  labels=['negative', 'neutral', 'positive'], average='weighted') )\n",
        "print(\"Recall is: \",    recall_score(df.manual_classification.astype(str), df.sentiment_label.astype(str),  labels=['negative', 'neutral', 'positive'], average='weighted')  )\n",
        "print(\"Kappa is: \",    cohen_kappa_score(df.manual_classification.astype(str), df.sentiment_label.astype(str)))"
      ],
      "metadata": {
        "id": "Z_VH0JlGDB0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relabelling the columns with only two values (negative, neutral/positive)\n",
        "df['manual_classification'] = df['goldstandard'].replace({0: 'negative', 1: 'neutral', 2: 'neutral'})\n",
        "df['sentiment_label'] = df['y_pred'].replace({'dangerous': 'negative', 'neutral': 'neutral'})\n",
        "\n",
        "# Printing the scores\n",
        "print(\"F1 Score is: \",  f1_score(df.manual_classification.astype(str), df.sentiment_label.astype(str), labels=['negative', 'neutral'], average='weighted'))\n",
        "print(\"Precision is: \", precision_score(df.manual_classification.astype(str), df.sentiment_label.astype(str),  labels=['negative', 'neutral'], average='weighted') )\n",
        "print(\"Recall is: \",    recall_score(df.manual_classification.astype(str), df.sentiment_label.astype(str),  labels=['negative', 'neutral'], average='weighted')  )\n",
        "print(\"Kappa is: \",    cohen_kappa_score(df.manual_classification.astype(str), df.sentiment_label.astype(str)))"
      ],
      "metadata": {
        "id": "bPXfoDQCHcao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Topic Model of Risks videos (LDA)**"
      ],
      "metadata": {
        "id": "g-sfv6MMQ3Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "file_name = 'drive/MyDrive/AI Risks Datasets/risks_comments.csv'\n",
        "df = pd.read_csv(file_name)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "s3s8cfc6Q6k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the not useful columns\n",
        "df = df.drop(columns=['channelId', 'videoId', 'textDisplay', 'authorDisplayName', 'authorProfileImageUrl', 'authorChannelUrl', 'authorChannelId.value', 'canRate', 'viewerRating', 'likeCount', 'publishedAt', 'updatedAt', 'id', 'parentId', 'moderationStatus', 'created_at'], axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2YTqa9FaRBKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the entries that are null (not string)\n",
        "df = df.loc[df['textOriginal'].apply(lambda x: isinstance(x, str))]\n",
        "len(df)"
      ],
      "metadata": {
        "id": "cINkX_unejFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter comments that explicitly mention AI\n",
        "keywords = ['AI', 'A.I.', 'A.I', 'artificial intelligence', 'AIs']\n",
        "pattern = '|'.join(keywords)\n",
        "df = df[df['textOriginal'].str.contains(pattern, case=False)]\n",
        "len(df)"
      ],
      "metadata": {
        "id": "PGq-lnf0ec78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punctuation/lower casing\n",
        "import re\n",
        "# Remove punctuation\n",
        "df['paper_text_processed'] = df['textOriginal'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "# Convert the titles to lowercase\n",
        "df['paper_text_processed'] = df['paper_text_processed'].map(lambda x: x.lower())\n",
        "# Print out the first rows of df\n",
        "df['paper_text_processed'].head()"
      ],
      "metadata": {
        "id": "XdPinHunRDjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Cloud\n",
        "# Import the wordcloud library\n",
        "from wordcloud import WordCloud\n",
        "# Join the different processed titles together.\n",
        "long_string = ','.join(list(df['paper_text_processed'].values))\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
        "# Generate a word cloud\n",
        "wordcloud.generate(long_string)\n",
        "# Visualize the word cloud\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "jMRsFfG6RHy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for LDA Analysis\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'to', 'subject', 're', 'one', 'like', 'video', 'comment', 'much', \\\n",
        "                   'please', 'lol', 'really', 'first', 'would', 'could', 'should', 'going', 'get', \\\n",
        "                   'take', 'are', 'is', 'what', 'know', 'even', 'something', 'way', 'really', 'say', \\\n",
        "                   'thing', 'anything', 'talk', 'actually', 'still', 'also', 'yet', 'let', 'make', \\\n",
        "                   'set', 'more', 'other', 'yes', 'no', 'im', 'thanks', 'thank', 'oh', 'ah', 'gonna', \\\n",
        "                   'yeah', 'ok', 'thought', 'tho', 'though', 'okay', 'look', 'much', 'looks', 'looking', \\\n",
        "                   'imma', 'hey', 'hi', 'likes', 'views', 'that', 'cant', 'doesnt', 'does', 'keep', 'tell', \\\n",
        "                   'dont', 'take', 'etc', 'say', 'says', 'said', 'told', 'well', 'just', 'come', 'came', \\\n",
        "                   'do', 'not', 'isnt', 'can', 'use', 'need', 'many', 'lot', 'made', 'want', 'think', 'will'])\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc))\n",
        "             if word not in stop_words] for doc in texts]\n",
        "data = df.paper_text_processed.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "# remove stop words\n",
        "data_words = remove_stopwords(data_words)\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "id": "bsI9g6vGRNig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.corpora as corpora\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "# Create Corpus\n",
        "texts = data_words\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "# View\n",
        "print(corpus[:1][0][:30])"
      ],
      "metadata": {
        "id": "GV_g148ORPlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA model training\n",
        "from pprint import pprint\n",
        "# number of topics\n",
        "num_topics = 10\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=num_topics)\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "metadata": {
        "id": "4UaSvPNxRQBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzing LDA model results\n",
        "!pip install pyLDAvis\n",
        "!pip install pandas==1.5.3\n",
        "!mkdir results\n",
        "import pyLDAvis.gensim\n",
        "import pickle\n",
        "import pyLDAvis\n",
        "import os\n",
        "\n",
        "# Visualize the topics\n",
        "pyLDAvis.enable_notebook()\n",
        "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
        "# # this is a bit time consuming - make the if statement True\n",
        "# # if you want to execute visualization prep yourself\n",
        "if 1 == 1:\n",
        "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
        "    with open(LDAvis_data_filepath, 'wb') as f:\n",
        "        pickle.dump(LDAvis_prepared, f)\n",
        "\n",
        "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
        "LDAvis_prepared"
      ],
      "metadata": {
        "id": "mNmS1ZnuRU4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Classification of Risks videos comments**"
      ],
      "metadata": {
        "id": "e_AbpeUw4rXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 1: bart-large-mnli"
      ],
      "metadata": {
        "id": "96V81zFT6pj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "labels = [ \"job loss\", \"privacy and security\", \"bias\", \"regulatory and legal challenges\", \\\n",
        "          \"manipulation and misinformation\", \"future catastrophic risks\", \"accidents\", \"plagiarism in art\", \"other\"]\n",
        "\n",
        "def analyse_sentiment(text):\n",
        "    output = pipe(text,\n",
        "                  candidate_labels = labels,\n",
        "                  hypothesis_template = 'This comment says AI is dangerous because of {}' # here you add the hypothesis - {} indicates where labels are added\n",
        "                 )\n",
        "    labs = output[\"labels\"]\n",
        "    return labs[0]"
      ],
      "metadata": {
        "id": "aNQbfHah4qqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with 100 entries\n",
        "%%time\n",
        "df_s = df.sample(n=100)\n",
        "df_s[\"y_pred\"] = df_s.textOriginal.apply(analyse_sentiment)"
      ],
      "metadata": {
        "id": "1PQucsJSL3j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the model on all the data\n",
        "%%time\n",
        "df[\"y_pred\"] = df.textOriginal.apply(analyse_sentiment)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "tv87q7m7EFyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relabelling the columns\n",
        "df['manual_classification'] = df['goldstandard'].replace({0: \"other\", 1: \"job loss\", 2: \"privacy and security\", 3: \"bias\", 4: \"regulatory and legal challenges\", \\\n",
        "          5: \"manipulation and misinformation\", 6: \"future catastrophic risks\", 7: \"accidents\", 8: \"plagiarism in art\"})\n",
        "df['sentiment_label'] = df['y_pred']\n",
        "\n",
        "# Printing the scores\n",
        "print(\"F1 Score is: \",  f1_score(df.manual_classification.astype(str), df.sentiment_label.astype(str), labels=[\"job loss\", \"privacy and security\", \"bias\", \"regulatory and legal challenges\", \"manipulation and misinformation\", \"future catastrophic risks\", \"accidents\", \"plagiarism in art\", \"other\"], average='weighted'))\n",
        "print(\"Precision is: \", precision_score(df.manual_classification.astype(str), df.sentiment_label.astype(str),  labels=[\"job loss\", \"privacy and security\", \"bias\", \"regulatory and legal challenges\", \"manipulation and misinformation\", \"future catastrophic risks\", \"accidents\", \"plagiarism in art\", \"other\"], average='weighted') )\n",
        "print(\"Recall is: \",    recall_score(df.manual_classification.astype(str), df.sentiment_label.astype(str),  labels=[\"job loss\", \"privacy and security\", \"bias\", \"regulatory and legal challenges\", \"manipulation and misinformation\", \"future catastrophic risks\", \"accidents\", \"plagiarism in art\", \"other\"], average='weighted')  )\n",
        "print(\"Kappa is: \",    cohen_kappa_score(df.manual_classification.astype(str), df.sentiment_label.astype(str)))"
      ],
      "metadata": {
        "id": "LWqWDP-hEJiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 2: Dictionary method"
      ],
      "metadata": {
        "id": "RfKoHSz76tlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the entries that are null (not string)\n",
        "df = df.loc[df['textOriginal'].apply(lambda x: isinstance(x, str))]\n",
        "# Filter comments that explicitly mention AI\n",
        "keywords = ['AI', 'A.I.', 'A.I', 'artificial intelligence', 'AIs']\n",
        "pattern = '|'.join(keywords)\n",
        "df = df[df['textOriginal'].str.contains(pattern, case=False)]\n",
        "len(df)"
      ],
      "metadata": {
        "id": "TcKq8HLQKJh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keywords for each category\n",
        "keywords_1 = ['Employment', 'Occupation', 'Career', 'Job', 'Economic impact', 'Labor', 'Labour', 'Unemployment', 'Workforce', 'Displacement']\n",
        "keywords_2 = ['Security', 'Privacy', 'Data protection', 'Confidentiality', 'Cybersecurity', 'Vulnerability', 'Vulnerabilities', 'Encryption', 'GDPR', 'Data sourcing', 'Hacking', 'Hacker', 'Open source']\n",
        "keywords_3 = ['Bias', 'Discrimination', 'Race', 'Discriminatory', 'Fairness', 'Social justice', 'Equity']\n",
        "keywords_4 = ['Regulatory', 'Legal', 'Corporation', 'Government', 'Gov', 'Law', 'Tax', 'Regulation', 'Liability', 'Accountability', 'Governance', 'Regulate']\n",
        "keywords_5 = ['Manipulation', 'Misinformation', 'Deepfake', 'Faux news', 'Fake news', 'Disinformation']\n",
        "keywords_6 = ['Future', 'Existential', 'Uncontrollable', 'Loss of control', 'Uncontrolled', 'Catastrophic', 'Catastrophe', 'Takeover', 'Out of control', 'Extinction', 'Control us', 'Apocalypse', 'Extinction', 'Apocalyptic', 'Screwed', 'Doomed', 'Depopulation', 'Destroy', 'Destruction', 'Annihilated', 'Disaster', 'Kill']\n",
        "keywords_7 = ['Accident', 'Autonomous vehicle', 'Car', 'Self-driving', 'Self driving', 'Crash', 'Incident', 'Road', 'Collision', 'Vehicle', 'Drive', 'Driving']\n",
        "keywords_8 = ['Plagiarism', 'Art', 'Artist', 'Artwork', 'Imitation', 'Copyright', 'Intellectual property', 'Ownership']\n",
        "\n",
        "keyword_list = [keywords_1, keywords_2, keywords_3, keywords_4, keywords_5, keywords_6, keywords_7, keywords_8]\n",
        "num_comments = [0, 0, 0, 0, 0, 0, 0, 0] # number of comments mentioning at least one of the keywords for each category\n",
        "\n",
        "# Convert text in 'textOriginal' column and keywords to lowercase\n",
        "df['textOriginal'] = df['textOriginal'].str.lower()\n",
        "for i, list in enumerate(keyword_list):\n",
        "  keyword_list[i] = [keyword.lower() for keyword in  keyword_list[i]]\n",
        "\n",
        "for i, list in enumerate(keyword_list):\n",
        "  num_comments[i] = df['textOriginal'].str.contains('|'.join(keyword_list[i])).sum()\n",
        "\n",
        "print(num_comments)"
      ],
      "metadata": {
        "id": "c4jzpjE66xYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "bar_titles = ['Job loss', 'Security', 'Bias', 'Regulations', 'Misinformation', 'Existential risks', 'Accidents', 'Plagiarism']\n",
        "sorted_data, sorted_titles = zip(*sorted(zip(num_comments, bar_titles)))\n",
        "\n",
        "plt.bar(sorted_titles, sorted_data, color='#4287f5')\n",
        "\n",
        "plt.xlabel('Categories')\n",
        "plt.ylabel('Number of videos')\n",
        "plt.title('Most discussed risks in the titles')\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dH7OnsCzT0WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Downloading the new dataset**"
      ],
      "metadata": {
        "id": "K9ykkn5aMDxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the new dataset\n",
        "file_name = 'risks_comments_labeled.csv'\n",
        "df_s.to_csv(file_name, index=False)\n",
        "files.download(file_name)"
      ],
      "metadata": {
        "id": "fgZY3tpA5dl4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "2b69ddf8-e54b-4385-9dc5-2bfc110ed722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_05b3c04d-250a-4f6b-b49c-ee6daef5643c\", \"ai_comments_labeled.csv\", 9984584)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}